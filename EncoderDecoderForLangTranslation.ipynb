{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaaccodekill/EncoderDecoder/blob/main/EncoderDecoderForLangTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSBFGOjpykxg"
      },
      "outputs": [],
      "source": [
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKt2p2l2zxqI"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FF9ztNXTxe3X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import lightning as L\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBI7DGcZu_wN"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "59gc5jRU0wTz"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apuVH9oF1qv6"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"bentrevett/multi30k\")  # splits: train/validation/test\n",
        "print(ds[\"train\"][0])  # {'en': 'A man ...', 'de': 'Ein Mann ...'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PNhDKYGN2H_r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NkLm_7342mOX"
      },
      "outputs": [],
      "source": [
        "spacy_en = spacy.load('en_core_web_sm', disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM_hSN6X3pL_"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FtwnFn1Y3SHn"
      },
      "outputs": [],
      "source": [
        "spacy_de = spacy.load('de_core_news_sm', disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0YBbP9x3M69"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9bnodTg44Ggw"
      },
      "outputs": [],
      "source": [
        "def tokenizer_ger(text):\n",
        "  return [w.text for w in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenizer_en(text):\n",
        "  return [w.text for w in spacy_en.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVOEBB4k5YcE"
      },
      "outputs": [],
      "source": [
        "# tokenize the dataset, add init token and eos tokens\n",
        "\n",
        "german = ds['train'].map(lambda x: {'german': ['<sos>'] + tokenizer_ger(x['de']) + ['<eos>']})\n",
        "english = ds['train'].map(lambda x: {'english': ['<sos>'] + tokenizer_en(x['en']) + ['<eos>']})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MuBffqU_SrS",
        "outputId": "2f9fcb8d-4759-4398-a7d1-61f3c62ae82e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['en', 'de']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "ds[\"train\"].column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkI9coyu5G4l"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEhrWDAh-js2"
      },
      "outputs": [],
      "source": [
        "# let's try to figure out how to tokenize a whole batch of training data for both languages\n",
        "\n",
        "SOS, EOS = '<sos>', '<eos>'\n",
        "\n",
        "def batch_tokenize(batch):\n",
        "  en_texts = batch[\"en\"]\n",
        "  de_texts = batch[\"de\"]\n",
        "\n",
        "  en_docs = list(spacy_en.pipe(en_texts))\n",
        "  de_docs = list(spacy_de.pipe(de_texts))\n",
        "\n",
        "  en_tokens = [[SOS] + [w.text.lower() for w in doc] + [EOS] for doc in en_docs]\n",
        "  de_tokens = [[SOS] + [w.text.lower() for w in doc] + [EOS] for doc in de_docs]\n",
        "\n",
        "  return {\"en\": en_tokens, \"de\": de_tokens}\n",
        "\n",
        "batch_tokenize(ds[\"train\"][:2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-Llc7IMATAC"
      },
      "outputs": [],
      "source": [
        "# lets do all for all the whole data\n",
        "\n",
        "ds_tokenized = ds.map(batch_tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20ghJfdkCTmF"
      },
      "outputs": [],
      "source": [
        "ds_tokenized[\"train\"][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FpPU4Z5UCggA"
      },
      "outputs": [],
      "source": [
        "# build a vocabulary for each language with a min repeat size of 2\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(tokenized_data, min_freq=2):\n",
        "    # Special tokens\n",
        "    special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "\n",
        "    # Count frequencies\n",
        "    token_counter = Counter()\n",
        "    for tokens in tokenized_data:\n",
        "        token_counter.update(tokens)\n",
        "\n",
        "    # Build vocab with special tokens first\n",
        "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "\n",
        "    # Add tokens that meet min frequency\n",
        "    idx = len(special_tokens)\n",
        "    for token, freq in token_counter.items():\n",
        "        if freq >= min_freq and token not in vocab:\n",
        "            vocab[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_Pw6-a3pHID7"
      },
      "outputs": [],
      "source": [
        "german_vocab = build_vocab(ds_tokenized[\"train\"][\"de\"])\n",
        "english_vocab = build_vocab(ds_tokenized[\"train\"][\"en\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bz_uruIB-eo5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XgZ9v0GHNei"
      },
      "outputs": [],
      "source": [
        "print(german_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQudMzwxHbOP"
      },
      "outputs": [],
      "source": [
        "print(english_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9GzF-E8eHflg"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x is a vector of indices\n",
        "    # eg \"Isaac is awesome\" -> \"[Isaac, is, awesome]\" -> \"[10, 2, 8]\" ->\n",
        "    # the shape of x is (seq_length, N) where N is the batch size\n",
        "\n",
        "    embedding = self.dropout(self.embedding(x)) # randomly zero things in the embedding. to prevent overfitting\n",
        "    # embedding shape: (seq_length, N, embedding_size)\n",
        "\n",
        "    outputs, (hidden, cell) = self.rnn(embedding)\n",
        "\n",
        "    return hidden, cell\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x, hidden, cell):\n",
        "    # takes in the hidden and cell state output from the Encoder, as well as one word at a time\n",
        "    # and for x the input we know it takes in a word\n",
        "    x = x.unsqueeze(0)\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    # embedding shape: (1, N, embedding_size)\n",
        "    outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "    # shape of output: (1, N, hidden_size)\n",
        "\n",
        "    predictions = self.fc(outputs)\n",
        "    # shape of predictions: (1, N, length_of_vocab)\n",
        "\n",
        "    predictions = predictions.squeeze(0)\n",
        "\n",
        "    return predictions, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4jO6J-C6l-c7"
      },
      "outputs": [],
      "source": [
        "# Lets create a lightning module to rule them all.\n",
        "\n",
        "class Seq2seq(L.LightningModule):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__() #\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.loss_func = nn.CrossEntropyLoss(ignore_index=english_vocab['<pad>']) # Ignore padding in loss calculation\n",
        "\n",
        "  def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "    # teacher force ratio is just saying 50% of the time, don't use the previously predicted word from the decoder as the next input to the decoder\n",
        "    # instead use the target (the correct target words)\n",
        "    batch_size = source.shape[1]\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(english_vocab) # Target language is English\n",
        "\n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
        "\n",
        "    hidden, cell = self.encoder(source)\n",
        "\n",
        "    # First input to the decoder is the <sos> token of the target language\n",
        "    x = target[0]\n",
        "\n",
        "    for t in range(1, target_len):\n",
        "      output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "      outputs[t] = output\n",
        "      best_guess = output.argmax(1)\n",
        "      # Use teacher forcing with a certain probability\n",
        "      x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    # Swap source and target to match the model: encode German (source), decode English (target)\n",
        "    source = batch[\"de\"]\n",
        "    target = batch[\"en\"]\n",
        "    outputs = self(source, target)\n",
        "    # Flatten the outputs and targets for loss calculation, excluding the first token (<sos>)\n",
        "    outputs = outputs[1:].reshape(-1, outputs.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "    loss = self.loss_func(outputs, target)\n",
        "    self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    # Swap source and target to match the model: encode German (source), decode English (target)\n",
        "    source = batch[\"de\"]\n",
        "    target = batch[\"en\"]\n",
        "    outputs = self(source, target, teacher_force_ratio=0) # No teacher forcing during validation\n",
        "    # Flatten the outputs and targets for loss calculation, excluding the first token (<sos>)\n",
        "    outputs = outputs[1:].reshape(-1, outputs.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "    loss = self.loss_func(outputs, target)\n",
        "    self.log(\"val_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MCRoNpAYqpcx"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(\n",
        "    input_size=len(german_vocab),\n",
        "    embedding_size=256,\n",
        "    hidden_size=1024,\n",
        "    num_layers=2,\n",
        "    p=0.5\n",
        ")\n",
        "\n",
        "decoder = Decoder(\n",
        "    input_size=len(english_vocab), # Input to decoder is from the target language (English)\n",
        "    embedding_size=256,\n",
        "    hidden_size=1024,\n",
        "    output_size=len(english_vocab), # Output of decoder is the target language vocabulary size (English)\n",
        "    num_layers=2,\n",
        "    p=0.5\n",
        ")\n",
        "\n",
        "\n",
        "model = Seq2seq(encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "A-tCP6Ifq1tt"
      },
      "outputs": [],
      "source": [
        "# # yay training finally\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     # Pad sequences to the maximum length in the batch\n",
        "#     german_batch = [torch.tensor([german_vocab[token] if token in german_vocab else german_vocab['<unk>'] for token in item[\"de\"]]) for item in batch]\n",
        "#     english_batch = [torch.tensor([english_vocab[token] if token in english_vocab else english_vocab['<unk>'] for token in item[\"en\"]]) for item in batch]\n",
        "\n",
        "#     padded_german = pad_sequence(german_batch, batch_first=False, padding_value=german_vocab['<pad>'])\n",
        "#     padded_english = pad_sequence(english_batch, batch_first=False, padding_value=english_vocab['<pad>'])\n",
        "\n",
        "#     return {\"de\": padded_german, \"en\": padded_english}\n",
        "\n",
        "\n",
        "# trainer = L.Trainer(max_epochs=20)\n",
        "# dataloader = DataLoader(ds_tokenized[\"train\"], batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "# val_dataloader = DataLoader(ds_tokenized[\"validation\"], batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "# trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O37658n-mNJ"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def preprocess_to_indices(dataset):\n",
        "    def convert_to_indices(examples):\n",
        "        de_indices = []\n",
        "        en_indices = []\n",
        "\n",
        "        for de_tokens, en_tokens in zip(examples['de'], examples['en']):\n",
        "            de_idx = [german_vocab.get(token, german_vocab['<unk>']) for token in de_tokens]\n",
        "            en_idx = [english_vocab.get(token, english_vocab['<unk>']) for token in en_tokens]\n",
        "            de_indices.append(de_idx)\n",
        "            en_indices.append(en_idx)\n",
        "\n",
        "        return {'de_indices': de_indices, 'en_indices': en_indices}\n",
        "\n",
        "    return dataset.map(convert_to_indices, batched=True, remove_columns=['de', 'en'])\n",
        "\n",
        "ds_indexed = preprocess_to_indices(ds_tokenized)\n",
        "\n",
        "\n",
        "def collate_fn_fast(batch):\n",
        "    german_batch = [torch.tensor(item[\"de_indices\"]) for item in batch]\n",
        "    english_batch = [torch.tensor(item[\"en_indices\"]) for item in batch]\n",
        "\n",
        "    padded_german = pad_sequence(german_batch, batch_first=False, padding_value=german_vocab['<pad>'])\n",
        "    padded_english = pad_sequence(english_batch, batch_first=False, padding_value=english_vocab['<pad>'])\n",
        "\n",
        "    return {\"de\": padded_german, \"en\": padded_english}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuuvf3tw_aQ_"
      },
      "outputs": [],
      "source": [
        "trainer = L.Trainer(max_epochs=20, log_every_n_steps=10, accelerator=\"gpu\",devices=1,precision=\"16-mixed\")\n",
        "dataloader = DataLoader(ds_indexed[\"train\"], batch_size=64, shuffle=True, collate_fn=collate_fn_fast)\n",
        "val_dataloader = DataLoader(ds_indexed[\"validation\"], batch_size=64, shuffle=False, collate_fn=collate_fn_fast)\n",
        "trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=\"lightning_logs/\""
      ],
      "metadata": {
        "id": "AzDGFLpDHiyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path"
      ],
      "metadata": {
        "id": "f2CCUpJSPOOu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(path_to_best_checkpoint)"
      ],
      "metadata": {
        "id": "oSLNOLk2QlFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train() # Set the model back to training mode after loading the checkpoint\n",
        "trainer = L.Trainer(max_epochs=30, log_every_n_steps=10, accelerator=\"gpu\",devices=1,precision=\"16-mixed\")\n",
        "dataloader = DataLoader(ds_indexed[\"train\"], batch_size=64, shuffle=True, collate_fn=collate_fn_fast)\n",
        "val_dataloader = DataLoader(ds_indexed[\"validation\"], batch_size=64, shuffle=False, collate_fn=collate_fn_fast)\n",
        "trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=val_dataloader, ckpt_path=path_to_best_checkpoint)"
      ],
      "metadata": {
        "id": "yklK1asKPW4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence, german_vocab, english_vocab, device, max_length=50):\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Tokenize the input sentence\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = ['<sos>'] + [token.lower() for token in sentence.split()] + ['<eos>']\n",
        "    else:\n",
        "        tokens = sentence  # Already tokenized\n",
        "\n",
        "    # Convert tokens to indices\n",
        "    indices = [german_vocab.get(token, german_vocab['<unk>']) for token in tokens]\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    sentence_tensor = torch.LongTensor(indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Get encoder output\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(sentence_tensor)\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    # Start with <sos> token\n",
        "    input_token = english_vocab['<sos>']\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        input_tensor = torch.LongTensor([input_token]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(input_tensor, hidden, cell)\n",
        "\n",
        "        # Get the predicted token\n",
        "        predicted = output.argmax(1).item()\n",
        "        outputs.append(predicted)\n",
        "\n",
        "        # Stop if we predict <eos>\n",
        "        if predicted == english_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "        input_token = predicted\n",
        "\n",
        "    # Convert indices back to words\n",
        "    english_idx2token = {idx: token for token, idx in english_vocab.items()}\n",
        "    translated_tokens = [english_idx2token.get(idx, '<unk>') for idx in outputs]\n",
        "\n",
        "    # Remove <eos> if present\n",
        "    if '<eos>' in translated_tokens:\n",
        "        translated_tokens = translated_tokens[:translated_tokens.index('<eos>')]\n",
        "\n",
        "    return ' '.join(translated_tokens)\n",
        "\n",
        "# Use the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Translate a sentence\n",
        "german_sentence = \"Ich zog das Schwert aus dem Stein\"\n",
        "translation = translate_sentence(model, german_sentence, german_vocab, english_vocab, device)\n",
        "print(f\"German: {german_sentence}\")\n",
        "print(f\"English: {translation}\")"
      ],
      "metadata": {
        "id": "25oBGaS0MrQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO1+aNK5NAZnIufbfdB79mM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}