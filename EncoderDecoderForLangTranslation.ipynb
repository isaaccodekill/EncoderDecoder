{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaaccodekill/EncoderDecoder/blob/main/EncoderDecoderForLangTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSBFGOjpykxg",
        "outputId": "92d8900a-dcc1-470e-c358-955a223f6c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.5.3-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.12/dist-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2025.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (25.0)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (2.8.0+cu126)\n",
            "Collecting torchmetrics<3.0,>0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>4.5.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (4.15.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.5.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.12.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.19.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics<3.0,>0.7.0->lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.20.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (3.10)\n",
            "Downloading lightning-2.5.3-py3-none-any.whl (824 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m824.2/824.2 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.3-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.2/828.2 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.5.3 lightning-utilities-0.15.2 pytorch-lightning-2.5.3 torchmetrics-1.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKt2p2l2zxqI",
        "outputId": "fbb9d813-daf9-478e-cf56-0fc8ebc97876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.32.4)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FF9ztNXTxe3X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import lightning as L\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBI7DGcZu_wN",
        "outputId": "62fc0b32-ccf2-41e1-8959-da97c15cdab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "59gc5jRU0wTz"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apuVH9oF1qv6"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"bentrevett/multi30k\")  # splits: train/validation/test\n",
        "print(ds[\"train\"][0])  # {'en': 'A man ...', 'de': 'Ein Mann ...'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PNhDKYGN2H_r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NkLm_7342mOX"
      },
      "outputs": [],
      "source": [
        "spacy_en = spacy.load('en_core_web_sm', disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM_hSN6X3pL_"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FtwnFn1Y3SHn"
      },
      "outputs": [],
      "source": [
        "spacy_de = spacy.load('de_core_news_sm', disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0YBbP9x3M69"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9bnodTg44Ggw"
      },
      "outputs": [],
      "source": [
        "def tokenizer_ger(text):\n",
        "  return [w.text for w in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenizer_en(text):\n",
        "  return [w.text for w in spacy_en.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVOEBB4k5YcE"
      },
      "outputs": [],
      "source": [
        "# tokenize the dataset, add init token and eos tokens\n",
        "\n",
        "german = ds['train'].map(lambda x: {'german': ['<sos>'] + tokenizer_ger(x['de']) + ['<eos>']})\n",
        "english = ds['train'].map(lambda x: {'english': ['<sos>'] + tokenizer_en(x['en']) + ['<eos>']})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MuBffqU_SrS",
        "outputId": "2f9fcb8d-4759-4398-a7d1-61f3c62ae82e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['en', 'de']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "ds[\"train\"].column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkI9coyu5G4l"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEhrWDAh-js2"
      },
      "outputs": [],
      "source": [
        "# let's try to figure out how to tokenize a whole batch of training data for both languages\n",
        "\n",
        "SOS, EOS = '<sos>', '<eos>'\n",
        "\n",
        "def batch_tokenize(batch):\n",
        "  en_texts = batch[\"en\"]\n",
        "  de_texts = batch[\"de\"]\n",
        "\n",
        "  en_docs = list(spacy_en.pipe(en_texts))\n",
        "  de_docs = list(spacy_de.pipe(de_texts))\n",
        "\n",
        "  en_tokens = [[SOS] + [w.text.lower() for w in doc] + [EOS] for doc in en_docs]\n",
        "  de_tokens = [[SOS] + [w.text.lower() for w in doc] + [EOS] for doc in de_docs]\n",
        "\n",
        "  return {\"en\": en_tokens, \"de\": de_tokens}\n",
        "\n",
        "batch_tokenize(ds[\"train\"][:2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-Llc7IMATAC"
      },
      "outputs": [],
      "source": [
        "# lets do all for all the whole data\n",
        "\n",
        "ds_tokenized = ds.map(batch_tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20ghJfdkCTmF"
      },
      "outputs": [],
      "source": [
        "ds_tokenized[\"train\"][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FpPU4Z5UCggA"
      },
      "outputs": [],
      "source": [
        "# build a vocabulary for each language with a min repeat size of 2\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(tokenized_data, min_freq=2):\n",
        "    # Special tokens\n",
        "    special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "\n",
        "    # Count frequencies\n",
        "    token_counter = Counter()\n",
        "    for tokens in tokenized_data:\n",
        "        token_counter.update(tokens)\n",
        "\n",
        "    # Build vocab with special tokens first\n",
        "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "\n",
        "    # Add tokens that meet min frequency\n",
        "    idx = len(special_tokens)\n",
        "    for token, freq in token_counter.items():\n",
        "        if freq >= min_freq and token not in vocab:\n",
        "            vocab[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_Pw6-a3pHID7"
      },
      "outputs": [],
      "source": [
        "german_vocab = build_vocab(ds_tokenized[\"train\"][\"de\"])\n",
        "english_vocab = build_vocab(ds_tokenized[\"train\"][\"en\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bz_uruIB-eo5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XgZ9v0GHNei"
      },
      "outputs": [],
      "source": [
        "print(german_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQudMzwxHbOP"
      },
      "outputs": [],
      "source": [
        "print(english_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9GzF-E8eHflg"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x is a vector of indices\n",
        "    # eg \"Isaac is awesome\" -> \"[Isaac, is, awesome]\" -> \"[10, 2, 8]\" ->\n",
        "    # the shape of x is (seq_length, N) where N is the batch size\n",
        "\n",
        "    embedding = self.dropout(self.embedding(x)) # randomly zero things in the embedding. to prevent overfitting\n",
        "    # embedding shape: (seq_length, N, embedding_size)\n",
        "\n",
        "    outputs, (hidden, cell) = self.rnn(embedding)\n",
        "\n",
        "    return hidden, cell\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x, hidden, cell):\n",
        "    # takes in the hidden and cell state output from the Encoder, as well as one word at a time\n",
        "    # and for x the input we know it takes in a word\n",
        "    x = x.unsqueeze(0)\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    # embedding shape: (1, N, embedding_size)\n",
        "    outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "    # shape of output: (1, N, hidden_size)\n",
        "\n",
        "    predictions = self.fc(outputs)\n",
        "    # shape of predictions: (1, N, length_of_vocab)\n",
        "\n",
        "    predictions = predictions.squeeze(0)\n",
        "\n",
        "    return predictions, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4jO6J-C6l-c7"
      },
      "outputs": [],
      "source": [
        "# Lets create a lightning module to rule them all.\n",
        "\n",
        "class Seq2seq(L.LightningModule):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super().__init__() #\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.loss_func = nn.CrossEntropyLoss(ignore_index=english_vocab['<pad>']) # Ignore padding in loss calculation\n",
        "\n",
        "  def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "    # teacher force ratio is just saying 50% of the time, don't use the previously predicted word from the decoder as the next input to the decoder\n",
        "    # instead use the target (the correct target words)\n",
        "    batch_size = source.shape[1]\n",
        "    target_len = target.shape[0]\n",
        "    target_vocab_size = len(english_vocab) # Target language is English\n",
        "\n",
        "    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
        "\n",
        "    hidden, cell = self.encoder(source)\n",
        "\n",
        "    # First input to the decoder is the <sos> token of the target language\n",
        "    x = target[0]\n",
        "\n",
        "    for t in range(1, target_len):\n",
        "      output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "      outputs[t] = output\n",
        "      best_guess = output.argmax(1)\n",
        "      # Use teacher forcing with a certain probability\n",
        "      x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    # Swap source and target to match the model: encode German (source), decode English (target)\n",
        "    source = batch[\"de\"]\n",
        "    target = batch[\"en\"]\n",
        "    outputs = self(source, target)\n",
        "    # Flatten the outputs and targets for loss calculation, excluding the first token (<sos>)\n",
        "    outputs = outputs[1:].reshape(-1, outputs.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "    loss = self.loss_func(outputs, target)\n",
        "    self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    # Swap source and target to match the model: encode German (source), decode English (target)\n",
        "    source = batch[\"de\"]\n",
        "    target = batch[\"en\"]\n",
        "    outputs = self(source, target, teacher_force_ratio=0) # No teacher forcing during validation\n",
        "    # Flatten the outputs and targets for loss calculation, excluding the first token (<sos>)\n",
        "    outputs = outputs[1:].reshape(-1, outputs.shape[2])\n",
        "    target = target[1:].reshape(-1)\n",
        "    loss = self.loss_func(outputs, target)\n",
        "    self.log(\"val_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MCRoNpAYqpcx"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(\n",
        "    input_size=len(german_vocab),\n",
        "    embedding_size=256,\n",
        "    hidden_size=1024,\n",
        "    num_layers=2,\n",
        "    p=0.5\n",
        ")\n",
        "\n",
        "decoder = Decoder(\n",
        "    input_size=len(english_vocab), # Input to decoder is from the target language (English)\n",
        "    embedding_size=256,\n",
        "    hidden_size=1024,\n",
        "    output_size=len(english_vocab), # Output of decoder is the target language vocabulary size (English)\n",
        "    num_layers=2,\n",
        "    p=0.5\n",
        ")\n",
        "\n",
        "\n",
        "model = Seq2seq(encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "A-tCP6Ifq1tt"
      },
      "outputs": [],
      "source": [
        "# # yay training finally\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     # Pad sequences to the maximum length in the batch\n",
        "#     german_batch = [torch.tensor([german_vocab[token] if token in german_vocab else german_vocab['<unk>'] for token in item[\"de\"]]) for item in batch]\n",
        "#     english_batch = [torch.tensor([english_vocab[token] if token in english_vocab else english_vocab['<unk>'] for token in item[\"en\"]]) for item in batch]\n",
        "\n",
        "#     padded_german = pad_sequence(german_batch, batch_first=False, padding_value=german_vocab['<pad>'])\n",
        "#     padded_english = pad_sequence(english_batch, batch_first=False, padding_value=english_vocab['<pad>'])\n",
        "\n",
        "#     return {\"de\": padded_german, \"en\": padded_english}\n",
        "\n",
        "\n",
        "# trainer = L.Trainer(max_epochs=20)\n",
        "# dataloader = DataLoader(ds_tokenized[\"train\"], batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "# val_dataloader = DataLoader(ds_tokenized[\"validation\"], batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
        "# trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O37658n-mNJ"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def preprocess_to_indices(dataset):\n",
        "    def convert_to_indices(examples):\n",
        "        de_indices = []\n",
        "        en_indices = []\n",
        "\n",
        "        for de_tokens, en_tokens in zip(examples['de'], examples['en']):\n",
        "            de_idx = [german_vocab.get(token, german_vocab['<unk>']) for token in de_tokens]\n",
        "            en_idx = [english_vocab.get(token, english_vocab['<unk>']) for token in en_tokens]\n",
        "            de_indices.append(de_idx)\n",
        "            en_indices.append(en_idx)\n",
        "\n",
        "        return {'de_indices': de_indices, 'en_indices': en_indices}\n",
        "\n",
        "    return dataset.map(convert_to_indices, batched=True, remove_columns=['de', 'en'])\n",
        "\n",
        "ds_indexed = preprocess_to_indices(ds_tokenized)\n",
        "\n",
        "\n",
        "def collate_fn_fast(batch):\n",
        "    german_batch = [torch.tensor(item[\"de_indices\"]) for item in batch]\n",
        "    english_batch = [torch.tensor(item[\"en_indices\"]) for item in batch]\n",
        "\n",
        "    padded_german = pad_sequence(german_batch, batch_first=False, padding_value=german_vocab['<pad>'])\n",
        "    padded_english = pad_sequence(english_batch, batch_first=False, padding_value=english_vocab['<pad>'])\n",
        "\n",
        "    return {\"de\": padded_german, \"en\": padded_english}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuuvf3tw_aQ_"
      },
      "outputs": [],
      "source": [
        "trainer = L.Trainer(max_epochs=20, log_every_n_steps=10, accelerator=\"gpu\",devices=1,precision=\"16-mixed\")\n",
        "dataloader = DataLoader(ds_indexed[\"train\"], batch_size=64, shuffle=True, collate_fn=collate_fn_fast)\n",
        "val_dataloader = DataLoader(ds_indexed[\"validation\"], batch_size=64, shuffle=False, collate_fn=collate_fn_fast)\n",
        "trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=\"lightning_logs/\""
      ],
      "metadata": {
        "id": "AzDGFLpDHiyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path"
      ],
      "metadata": {
        "id": "f2CCUpJSPOOu"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(path_to_best_checkpoint)"
      ],
      "metadata": {
        "id": "oSLNOLk2QlFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train() # Set the model back to training mode after loading the checkpoint\n",
        "trainer = L.Trainer(max_epochs=30, log_every_n_steps=10, accelerator=\"gpu\",devices=1,precision=\"16-mixed\")\n",
        "dataloader = DataLoader(ds_indexed[\"train\"], batch_size=64, shuffle=True, collate_fn=collate_fn_fast)\n",
        "val_dataloader = DataLoader(ds_indexed[\"validation\"], batch_size=64, shuffle=False, collate_fn=collate_fn_fast)\n",
        "trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=val_dataloader, ckpt_path=path_to_best_checkpoint)"
      ],
      "metadata": {
        "id": "yklK1asKPW4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence, german_vocab, english_vocab, device, max_length=50):\n",
        "    model.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Tokenize the input sentence\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = ['<sos>'] + [token.lower() for token in sentence.split()] + ['<eos>']\n",
        "    else:\n",
        "        tokens = sentence  # Already tokenized\n",
        "\n",
        "    # Convert tokens to indices\n",
        "    indices = [german_vocab.get(token, german_vocab['<unk>']) for token in tokens]\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    sentence_tensor = torch.LongTensor(indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Get encoder output\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(sentence_tensor)\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    # Start with <sos> token\n",
        "    input_token = english_vocab['<sos>']\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        input_tensor = torch.LongTensor([input_token]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(input_tensor, hidden, cell)\n",
        "\n",
        "        # Get the predicted token\n",
        "        predicted = output.argmax(1).item()\n",
        "        outputs.append(predicted)\n",
        "\n",
        "        # Stop if we predict <eos>\n",
        "        if predicted == english_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "        input_token = predicted\n",
        "\n",
        "    # Convert indices back to words\n",
        "    english_idx2token = {idx: token for token, idx in english_vocab.items()}\n",
        "    translated_tokens = [english_idx2token.get(idx, '<unk>') for idx in outputs]\n",
        "\n",
        "    # Remove <eos> if present\n",
        "    if '<eos>' in translated_tokens:\n",
        "        translated_tokens = translated_tokens[:translated_tokens.index('<eos>')]\n",
        "\n",
        "    return ' '.join(translated_tokens)\n",
        "\n",
        "# Use the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Translate a sentence\n",
        "german_sentence = \"Ich zog das Schwert aus dem Stein\"\n",
        "translation = translate_sentence(model, german_sentence, german_vocab, english_vocab, device)\n",
        "print(f\"German: {german_sentence}\")\n",
        "print(f\"English: {translation}\")"
      ],
      "metadata": {
        "id": "25oBGaS0MrQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO1+aNK5NAZnIufbfdB79mM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}